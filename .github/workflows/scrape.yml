name: Scrape Runner

on:
  workflow_dispatch:
    inputs:
      job_id:
        description: 'Supabase job ID for tracking'
        required: true
        type: string
      skus:
        description: 'Comma-separated list of SKUs to scrape'
        required: false
        type: string
      scrapers:
        description: 'Comma-separated list of scraper names to run'
        required: false
        type: string
      test_mode:
        description: 'Run in test mode'
        required: false
        type: boolean
        default: false
      max_workers:
        description: 'Maximum number of concurrent workers'
        required: false
        type: number
        default: 3
      mode:
        description: 'Runner mode: full (process all SKUs) or chunk_worker (claim chunks from queue)'
        required: false
        type: string
        default: 'full'

# Only run on self-hosted runners with Docker
jobs:
  scrape:
    runs-on: [self-hosted, docker]
    timeout-minutes: 60
    environment: production

    env:
      # API Key authentication (simple, no token refresh needed)
      SCRAPER_API_URL: ${{ secrets.SCRAPER_API_URL }}
      SCRAPER_API_KEY: ${{ secrets.SCRAPER_API_KEY }}
      # Webhook secret for HMAC fallback (Docker crash reporting)
      SCRAPER_WEBHOOK_SECRET: ${{ secrets.SCRAPER_WEBHOOK_SECRET }}
      # Callback URL for HMAC fallback
      CALLBACK_URL: ${{ secrets.SCRAPER_CALLBACK_URL }}

      JOB_ID: ${{ inputs.job_id }}
      MODE: ${{ inputs.mode }}

    steps:
      - name: Run scraping job in Docker
        id: scrape
        env:
          RUNNER_NAME: ${{ runner.name }}
        run: |
          echo "Starting job: $JOB_ID"
          echo "Runner: $RUNNER_NAME"
          echo "Mode: $MODE"
          
          # Run the API-driven runner module
          # The runner authenticates with API key, fetches config, executes scraping, and submits results
          docker run --rm \
            -e SCRAPER_API_URL="$SCRAPER_API_URL" \
            -e SCRAPER_API_KEY="$SCRAPER_API_KEY" \
            -e RUNNER_NAME="$RUNNER_NAME" \
            baystate-scraper:latest \
            python -m scraper_backend.runner --job-id "$JOB_ID" --runner-name "$RUNNER_NAME" --mode "$MODE" \
            > /tmp/scrape_results.json

      - name: Report failure via HMAC
        if: failure()
        env:
          RUNNER_NAME: ${{ runner.name }}
        run: |
          # This step only runs if Docker itself fails (out of memory, crash, etc.)
          # Uses HMAC signature since we can't use the Python API client
          PAYLOAD=$(jq -n \
            --arg job_id "$JOB_ID" \
            --arg runner_name "$RUNNER_NAME" \
            '{job_id: $job_id, status: "failed", runner_name: $runner_name, error_message: "Docker container failed - check GitHub Actions logs"}')
          
          SIGNATURE=$(echo -n "$PAYLOAD" | openssl dgst -sha256 -hmac "$SCRAPER_WEBHOOK_SECRET" | awk '{print $2}')
          
          curl -X POST "$CALLBACK_URL" \
            -H "Content-Type: application/json" \
            -H "X-Webhook-Signature: $SIGNATURE" \
            -d "$PAYLOAD"

